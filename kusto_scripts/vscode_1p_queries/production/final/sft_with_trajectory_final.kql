// =============================================================================
// SFT WITH FULL TRAJECTORY - FINAL VERSION
// =============================================================================
// PURPOSE: Extract complete conversations with FULL per-LLM-call trajectory
//          breakdown, including correct truncation detection.
//
// USE CASE: 
// - Detailed token analysis at each LLM decision point
// - Understanding how context grows within a turn
// - Detecting when truncation occurs
//
// ⚠️ MEMORY WARNING: 
// - The mv-expand on messagesJson is expensive
// - For large time windows (>6h), may hit E_LOW_MEMORY_CONDITION
// - Use sft_simple_final.kql if you don't need trajectory breakdown
//
// KEY FIELDS EXPLAINED:
// ─────────────────────────────────────────────────────────────────────────────
// FROM LLM API (ACTUAL - use for cost):
//   promptTokens      - What you're charged
//   completionTokens  - Model's response tokens
//   maxTokenWindow    - Model's context limit
//
// FROM COPILOT (ESTIMATES - use for analysis):
//   systemTokens      - System prompt + tool definitions (CONSTANT in turn)
//   userTokens        - User's message (CONSTANT in turn)
//   assistantTokens   - Prior responses (GROWS in turn)
//   toolResultTokens  - Tool outputs (GROWS in turn)
//   trajectoryTotal   - Sum of above
//
// TRUNCATION DETECTION:
//   exceededWindow    - TRUE if trajectoryTotal > maxTokenWindow
//                       This is the ONLY reliable truncation indicator!
//   tokenizerRatio    - promptTokens / trajectoryTotal (~0.4-0.8 typical)
//                       Shows tokenizer mismatch, NOT truncation
//
// DOCUMENTATION: See ../docs/vscode_1p_data_team_docs/understand_data_schema.md
// =============================================================================

// MEMORY: Reduce time window if you get E_LOW_MEMORY_CONDITION
let timeStart = ago(6h);
let timeEnd = now();
let minTurns = 3;
let maxTurns = 10;

// =============================================================================
// STEP 1: Get deduplicated conversation messages
// =============================================================================
let rawMessages = 
    AppEvents
    | where TimeGenerated > timeStart and TimeGenerated <= timeEnd
    | where Name == "GitHub.copilot-chat/conversation.messageText"
    | extend mode = tostring(Properties["mode"])
    | where mode == "agent"
    | extend conversationId = tostring(Properties["conversationId"])
    | extend messageId = tostring(Properties["messageId"])
    | extend source = tostring(Properties["source"])
    | extend messageText = tostring(Properties["messageText"])
    | extend userName = tostring(Properties["common.userName"])
    | project TimeGenerated, conversationId, messageId, source, messageText, userName;

let dedupedByMessageId = 
    rawMessages
    | summarize arg_max(strlen(messageText), TimeGenerated, messageText, userName)
        by conversationId, messageId, source
    | where isnotempty(messageText);

let userMsgs = dedupedByMessageId 
    | where source == "user" 
    | project conversationId, messageId, userMessage = messageText, userName;

let modelMsgs = dedupedByMessageId 
    | where source == "model"
    | summarize arg_min(TimeGenerated, messageText) by conversationId, messageId
    | project conversationId, messageId, modelMessage = messageText;

// =============================================================================
// STEP 2: Get token data (OUTPUT direction) with callIndex
// =============================================================================
let tokenEventsRaw = 
    AppEvents
    | where TimeGenerated > timeStart and TimeGenerated <= timeEnd
    | where Name == "GitHub.copilot-chat/engine.messages.length"
    | extend message_direction = tostring(Properties["message_direction"])
    | where message_direction == "output"
    | extend messageId = tostring(Properties["headerRequestId"])
    | extend model = tostring(Properties["baseModel"])
    | extend promptTokens = toint(Measurements["promptTokens"])
    | extend completionTokens = toint(Measurements["completionTokens"])
    | extend maxTokenWindow = toint(Measurements["maxTokenWindow"])
    | project TimeGenerated, messageId, model, promptTokens, completionTokens, maxTokenWindow
    | order by messageId, TimeGenerated asc;

// Add callIndex within each messageId (1, 2, 3, ...)
let tokenEvents = 
    tokenEventsRaw
    | extend callIndex = row_number(1, messageId != prev(messageId));

// =============================================================================
// STEP 3: Get trajectory (INPUT direction) with callIndex
// MEMORY OPTIMIZATION: Filter early, skip huge trajectories
// =============================================================================
let trajectoryEventsRaw = 
    AppEvents
    | where TimeGenerated > timeStart and TimeGenerated <= timeEnd
    | where Name == "GitHub.copilot-chat/engine.messages.length"
    | extend message_direction = tostring(Properties["message_direction"])
    | where message_direction == "input"
    | extend messageId = tostring(Properties["headerRequestId"])
    | extend messagesJsonStr = tostring(Properties["messagesJson"])
    | where isnotempty(messagesJsonStr)
    | where strlen(messagesJsonStr) < 500000  // Skip huge trajectories (OOM protection)
    | project TimeGenerated, messageId, messagesJsonStr
    | order by messageId, TimeGenerated asc;

let trajectoryEvents = 
    trajectoryEventsRaw
    | extend callIndex = row_number(1, messageId != prev(messageId));

// Parse trajectory breakdown per LLM call
let trajectoryParsed = 
    trajectoryEvents
    | mv-expand message = parse_json(messagesJsonStr)
    | extend role = tostring(message.role)
    | extend tokenCount = toint(message.content)
    | summarize 
        systemTokens = sumif(tokenCount, role == "system"),
        userTokens = sumif(tokenCount, role == "user"),
        assistantTokens = sumif(tokenCount, role == "assistant"),
        toolResultTokens = sumif(tokenCount, role == "tool"),
        trajectoryTotal = sum(tokenCount)
        by messageId, callIndex;

// =============================================================================
// STEP 4: Join OUTPUT with INPUT on (messageId, callIndex)
// =============================================================================
let tokenDataWithTrajectory = 
    tokenEvents
    | join kind=leftouter trajectoryParsed on messageId, callIndex
    | project-away messageId1, callIndex1
    // Coalesce nulls
    | extend systemTokens = coalesce(systemTokens, 0)
    | extend userTokens = coalesce(userTokens, 0)
    | extend assistantTokens = coalesce(assistantTokens, 0)
    | extend toolResultTokens = coalesce(toolResultTokens, 0)
    | extend trajectoryTotal = coalesce(trajectoryTotal, 0)
    // Calculate metrics
    | extend hasTrajectory = trajectoryTotal > 0
    // TRUNCATION: Only exceededWindow is reliable!
    | extend exceededWindow = trajectoryTotal > maxTokenWindow
    // TOKENIZER RATIO: Shows difference between Copilot estimate and actual API
    | extend tokenizerRatio = iif(trajectoryTotal > 0, round(todouble(promptTokens) / todouble(trajectoryTotal), 2), 0.0);

// Aggregate into llmCalls array per messageId (turn)
let tokenData = 
    tokenDataWithTrajectory
    | summarize 
        llmCalls = make_list(pack(
            "callIndex", callIndex,
            // From LLM API (ACTUAL):
            "promptTokens", promptTokens,
            "completionTokens", completionTokens,
            "model", model,
            "maxTokenWindow", maxTokenWindow,
            // From Copilot (ESTIMATES):
            "trajectory", pack(
                "systemTokens", systemTokens,        // CONSTANT within turn
                "userTokens", userTokens,            // CONSTANT within turn
                "assistantTokens", assistantTokens,  // GROWS within turn
                "toolResultTokens", toolResultTokens, // GROWS within turn
                "total", trajectoryTotal
            ),
            // Analysis:
            "hasTrajectory", hasTrajectory,
            "exceededWindow", exceededWindow,        // TRUE = truncation!
            "tokenizerRatio", tokenizerRatio
        )),
        // Turn-level summary
        turnMaxPromptTokens = max(promptTokens),
        turnMaxTrajectoryTotal = max(trajectoryTotal),
        turnMaxTokenWindow = max(maxTokenWindow),
        turnHasTruncation = max(toint(exceededWindow)) > 0,
        turnLlmCallCount = count()
        by messageId;

// =============================================================================
// STEP 5: Get tool metadata
// =============================================================================
let toolData = 
    AppEvents
    | where TimeGenerated > timeStart and TimeGenerated <= timeEnd
    | where Name == "GitHub.copilot-chat/toolCallDetailsInternal"
    | extend messageId = tostring(Properties["messageId"])
    | extend toolCounts = tostring(Properties["toolCounts"])
    | extend responseType = tostring(Properties["responseType"])
    | extend turnIndex = toint(Measurements["turnIndex"])
    | extend numRequests = toint(Measurements["numRequests"])
    | extend turnDuration = toint(Measurements["turnDuration"])
    | where responseType == "success"
    | project messageId, toolCounts, turnIndex, numRequests, turnDuration;

// =============================================================================
// STEP 6: Build turns
// =============================================================================
let turns = 
    userMsgs
    | join kind=inner modelMsgs on conversationId, messageId
    | join kind=inner tokenData on messageId
    | join kind=inner toolData on messageId
    | project-away conversationId1, messageId1, messageId2, messageId3
    | project 
        conversationId,
        userName,
        turnIndex,
        messageId,
        userMessage,
        modelMessage,
        llmCalls,
        turnSummary = pack(
            "maxPromptTokens", turnMaxPromptTokens,
            "maxTrajectoryTotal", turnMaxTrajectoryTotal,
            "maxTokenWindow", turnMaxTokenWindow,
            "hasTruncation", turnHasTruncation,
            "llmCallCount", turnLlmCallCount
        ),
        toolCounts,
        numRequests,
        turnDurationMs = turnDuration;

// =============================================================================
// STEP 7: Aggregate to conversations
// =============================================================================
let conversations = 
    turns
    | order by conversationId asc, turnIndex asc
    | summarize 
        userName = take_any(userName),
        capturedTurnCount = count(),
        minTurnIndex = min(turnIndex),
        maxTurnIndex = max(turnIndex),
        turnsArray = make_list(pack(
            "turnIndex", turnIndex,
            "messageId", messageId,
            "userMessage", userMessage,
            "modelMessage", modelMessage,
            "llmCalls", llmCalls,
            "turnSummary", turnSummary,
            "toolCounts", toolCounts,
            "numRequests", numRequests,
            "turnDurationMs", turnDurationMs
        ))
        by conversationId
    | mv-apply turnsArray on (
        order by toint(turnsArray.turnIndex) asc
        | summarize turnsArray = make_list(turnsArray)
    );

// =============================================================================
// STEP 8: Filter for complete conversations and output
// =============================================================================
conversations
| where minTurnIndex == 1
| where capturedTurnCount == maxTurnIndex
| where capturedTurnCount >= minTurns
| where capturedTurnCount <= maxTurns
| project 
    conversationId, 
    userName, 
    capturedTurnCount,
    minTurnIndex,
    maxTurnIndex,
    isComplete = true,
    turns = turnsArray
| order by capturedTurnCount desc
| take 50

// =============================================================================
// OUTPUT SCHEMA:
// {
//   "conversationId": "...",
//   "capturedTurnCount": 5,
//   "isComplete": true,
//   "turns": [
//     {
//       "turnIndex": 1,
//       "llmCalls": [
//         {
//           "callIndex": 1,
//           // From LLM API (ACTUAL):
//           "promptTokens": 24958,
//           "completionTokens": 172,
//           "model": "claude-opus-4.5",
//           "maxTokenWindow": 127997,
//           // From Copilot (ESTIMATES):
//           "trajectory": {
//             "systemTokens": 14722,      // CONSTANT within turn
//             "userTokens": 3916,         // CONSTANT within turn
//             "assistantTokens": 0,       // GROWS within turn
//             "toolResultTokens": 0,      // GROWS within turn
//             "total": 18638
//           },
//           // Analysis:
//           "hasTrajectory": true,
//           "exceededWindow": false,      // TRUE = truncation!
//           "tokenizerRatio": 1.34
//         },
//         {
//           "callIndex": 2,
//           "trajectory": {
//             "systemTokens": 14722,      // Same as call 1
//             "userTokens": 3916,         // Same as call 1
//             "assistantTokens": 144,     // Grew from call 1
//             "toolResultTokens": 1312,   // Grew from call 1
//             "total": 20094
//           }
//         }
//       ],
//       "turnSummary": {
//         "maxPromptTokens": 78559,
//         "maxTrajectoryTotal": 201547,
//         "maxTokenWindow": 127997,
//         "hasTruncation": true,        // TRUE if ANY call exceeded window
//         "llmCallCount": 9
//       }
//     }
//   ]
// }
// =============================================================================

