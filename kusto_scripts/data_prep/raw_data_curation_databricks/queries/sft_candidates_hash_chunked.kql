// =============================================================================
// SFT CANDIDATES - HASH-BASED CHUNKING (NO SAMPLING)
// =============================================================================
// 
// PURPOSE: Fetch ALL conversation candidates using hash-based chunking.
//          Guarantees conversation completeness by ensuring each conversation
//          is fully contained in exactly ONE chunk.
//
// CHUNKING: hash(conversationId) % {NUM_CHUNKS} == {CHUNK_NUM}
//           Replace {NUM_CHUNKS} and {CHUNK_NUM} in Python before execution.
//
// OUTPUT: All complete conversations matching criteria (no sampling).
//         Sampling is done in Python after aggregating all chunks.
//
// QUALITY FILTERS:
//   1. Completeness: minTurnIndex == 1 AND capturedTurnCount == maxTurnIndex
//   2. No failures: Excludes conversations with failed/cancelled/networkError turns
//   3. Token data: Only includes turns with valid LLM call data
//
// INCLUDES:
//   - Trajectory breakdown (systemTokens, userTokens, assistantTokens, toolResultTokens)
//   - Tool metadata (availableTools, toolCounts, numRequests, turnDurationMs)
//   - Turn summary (maxPromptTokens, totalCompletionTokens, llmCallCount, hasTrajectory)
//
// =============================================================================

// 15 days chosen for safe buffer (Dec 2024):
// - Provides ~335k bucketed conversations (2.8× the 120k target)
// - All buckets have 2× or more buffer for stratified sampling
let timeStart = ago(15d);
let timeEnd = now();

// Hash-based chunking parameters (replaced by Python)
let numChunks = {NUM_CHUNKS};  // Total number of chunks (e.g., 40)
let chunkNum = {CHUNK_NUM};    // Current chunk (0 to numChunks-1)

// BUCKET CONFIGURATION
let bucket_short_min = 3;
let bucket_short_max = 5;
let bucket_medium_min = 6;
let bucket_medium_max = 10;
let bucket_long_min = 11;
let bucket_long_max = 20;

// =============================================================================
// STEP 1: Get raw conversation messages WITH HASH FILTER
// =============================================================================
let rawMessages = 
    AppEvents
    | where TimeGenerated > timeStart and TimeGenerated <= timeEnd
    | where Name == "GitHub.copilot-chat/conversation.messageText"
    | extend mode = tostring(Properties["mode"])
    | where mode == "agent"
    | extend conversationId = tostring(Properties["conversationId"])
    // HASH-BASED CHUNKING: ensures entire conversation in one chunk
    | where hash(conversationId) % numChunks == chunkNum
    | extend messageId = tostring(Properties["messageId"])
    | extend source = tostring(Properties["source"])
    | extend messageText = tostring(Properties["messageText"])
    | extend userName = tostring(Properties["common.userName"])
    | project TimeGenerated, conversationId, messageId, source, messageText, userName;

let dedupedByMessageId = 
    rawMessages
    | summarize arg_max(strlen(messageText), TimeGenerated, messageText, userName)
        by conversationId, messageId, source
    | where isnotempty(messageText);

// Compute turnIndex using row_number() on user messages
let userMsgs = dedupedByMessageId 
    | where source == "user"
    | summarize turnIndex = count() by conversationId, messageId, messageText, userName
    | order by conversationId asc, messageId asc
    | serialize
    | extend turnIndex = row_number(1, prev(conversationId) != conversationId)
    | project conversationId, messageId, turnIndex, userMessage = messageText, userName;

let modelMsgs = dedupedByMessageId 
    | where source == "model"
    | summarize arg_min(TimeGenerated, messageText) by conversationId, messageId
    | project conversationId, messageId, modelMessage = messageText;

// =============================================================================
// STEP 2: Get token data (OUTPUT direction - actual API tokens)
// =============================================================================
let tokenEventsRaw = 
    AppEvents
    | where TimeGenerated > timeStart and TimeGenerated <= timeEnd
    | where Name == "GitHub.copilot-chat/engine.messages.length"
    | extend message_direction = tostring(Properties["message_direction"])
    | where message_direction == "output"  // IMPORTANT: lowercase "output"
    | extend messageId = tostring(Properties["headerRequestId"])  // IMPORTANT: use headerRequestId
    | extend conversationId = tostring(Properties["conversationId"])
    // Apply hash filter to token events too
    | where isnotempty(conversationId) and hash(conversationId) % numChunks == chunkNum
    | extend model = tostring(Properties["baseModel"])  // IMPORTANT: use baseModel
    | extend promptTokens = toint(Measurements["promptTokens"])
    | extend completionTokens = toint(Measurements["completionTokens"])
    | extend totalTokens = toint(Measurements["totalTokens"])
    | project TimeGenerated, messageId, model, promptTokens, completionTokens, totalTokens
    | order by messageId, TimeGenerated asc;

let tokenEvents = 
    tokenEventsRaw
    | serialize
    | extend callIndex = row_number(1, prev(messageId) != messageId);

// =============================================================================
// STEP 2.5: Get TRAJECTORY data (INPUT direction with messagesJson)
// Provides token breakdown by role (system, user, assistant, tool)
// =============================================================================
let trajectoryEvents = 
    AppEvents
    | where TimeGenerated > timeStart and TimeGenerated <= timeEnd
    | where Name == "GitHub.copilot-chat/engine.messages.length"
    | extend message_direction = tostring(Properties["message_direction"])
    | where message_direction == "input"
    | extend messageId = tostring(Properties["headerRequestId"])
    | extend conversationId = tostring(Properties["conversationId"])
    | where isnotempty(conversationId) and hash(conversationId) % numChunks == chunkNum
    | extend messagesJsonStr = tostring(Properties["messagesJson"])
    | where isnotempty(messagesJsonStr)
    | project TimeGenerated, messageId, messagesJsonStr
    | order by messageId, TimeGenerated asc
    | serialize
    | extend callIndex = row_number(1, prev(messageId) != messageId);

// Parse messagesJson to extract token breakdown by role
let trajectoryParsed = 
    trajectoryEvents
    | mv-expand message = parse_json(messagesJsonStr)
    | extend role = tostring(message.role)
    | extend tokenCount = toint(message.content)
    | summarize 
        systemTokens = sumif(tokenCount, role == "system"),
        userTokens = sumif(tokenCount, role == "user"),
        assistantTokens = sumif(tokenCount, role == "assistant"),
        toolResultTokens = sumif(tokenCount, role == "tool"),
        trajectoryTotal = sum(tokenCount)
        by messageId, callIndex;

// Join OUTPUT with INPUT on (messageId, callIndex)
let tokenDataWithTrajectory = 
    tokenEvents
    | join kind=leftouter trajectoryParsed on messageId, callIndex
    | project-away messageId1, callIndex1
    | extend systemTokens = coalesce(systemTokens, 0)
    | extend userTokens = coalesce(userTokens, 0)
    | extend assistantTokens = coalesce(assistantTokens, 0)
    | extend toolResultTokens = coalesce(toolResultTokens, 0)
    | extend trajectoryTotal = coalesce(trajectoryTotal, 0)
    | extend hasTrajectory = trajectoryTotal > 0;

// Aggregate token events to turn level with trajectory
// ─────────────────────────────────────────────────────────────────────────────
// TOKEN DATA STRUCTURE - Two separate objects per llmCall:
// ─────────────────────────────────────────────────────────────────────────────
//
// 1. actual_API: BILLED tokens from LLM API (OUTPUT direction)
//    - promptTokens: Input tokens charged (= system + user + assistant + toolResults)
//    - completionTokens: Output tokens from model
//    - totalTokens: promptTokens + completionTokens
//
// 2. estimates_Copilot: Client-side ESTIMATES (INPUT direction) - for truncation
//    - systemTokens: System prompt + tool definitions (CONSTANT within turn)
//    - userTokens: User message tokens (CONSTANT within turn)
//    - assistantTokens: Model's previous responses (GROWS within turn)
//    - toolResultTokens: Tool outputs accumulated (GROWS within turn)
//    - trajectoryTotal: Sum of all role estimates
//    - hasTrajectory: true if trajectory data was captured
//
// RELATIONSHIP:
//   actual_API.promptTokens ≈ estimates_Copilot.trajectoryTotal
//   (10-40% difference due to tokenizer mismatch - NOT truncation)
//
// FOR COST ANALYSIS: Use actual_API fields
// FOR COMPOSITION BREAKDOWN: Use estimates_Copilot fields
// ─────────────────────────────────────────────────────────────────────────────
let tokenData = 
    tokenDataWithTrajectory
    | summarize 
        llmCalls = make_list(pack(
            "callIndex", callIndex,
            "model", model,
            // ACTUAL tokens from LLM API (billed)
            "actual_API", pack(
                "promptTokens_(system+user+assistant+toolResults)", promptTokens,
                "completionTokens", completionTokens,
                "totalTokens", totalTokens
            ),
            // ESTIMATE tokens from Copilot client (trajectory breakdown)
            "estimates_Copilot", pack(
                "systemTokens", systemTokens,           // system prompt + tool definitions (CONSTANT within turn)
                "userTokens", userTokens,               // user message (CONSTANT within turn)
                "assistantTokens", assistantTokens,     // model responses (GROWS within turn)
                "toolResultTokens", toolResultTokens,   // tool outputs (GROWS within turn)
                "trajectoryTotal", trajectoryTotal,     // sum of all estimates
                "hasTrajectory", hasTrajectory
            )
        )),
        turnMaxPromptTokens = max(promptTokens),
        turnTotalCompletionTokens = sum(completionTokens),
        turnHasTrajectory = max(toint(hasTrajectory)) > 0
        by messageId;

// =============================================================================
// STEP 3: Get tool metadata (including availableTools - tool definitions)
// =============================================================================
// turnEndReason values:
//   - "stopped" (63.2%) ← ✅ Normal completion
//   - "toolUse" (28.0%) ← ✅ Tool was used, normal
//   - "cancelled" (5.2%) ← ❌ User cancelled - EXCLUDE
//   - "maxToolCalls" (3.1%) ← ⚠️ Hit tool limit - keep (still complete)
//   - "failed" (0.4%) ← ❌ LLM call failed - EXCLUDE
//   - "networkError" (0.2%) ← ❌ Network error - EXCLUDE
let toolData = 
    AppEvents
    | where TimeGenerated > timeStart and TimeGenerated <= timeEnd
    | where Name == "GitHub.copilot-chat/toolCallDetailsInternal"
    | extend messageId = tostring(Properties["messageId"])
    | extend conversationId = tostring(Properties["conversationId"])
    | where isnotempty(conversationId) and hash(conversationId) % numChunks == chunkNum
    | extend toolCounts = tostring(Properties["toolCounts"])
    | extend availableTools = tostring(Properties["availableTools"])  // JSON array of tool names (definitions available)
    | extend responseType = tostring(Properties["responseType"])
    | extend turnEndReason = tostring(Properties["turnEndReason"])
    | extend turnIndex = toint(Measurements["turnIndex"])
    | extend numRequests = toint(Measurements["numRequests"])
    | extend turnDuration = toint(Measurements["turnDuration"])
    | extend availableToolCount = toint(Measurements["availableToolCount"])
    | where responseType == "success"  // Only successful responses
    | project messageId, toolCounts, availableTools, availableToolCount, numRequests, turnDuration, turnEndReason;

// =============================================================================
// STEP 4: Build turns with all data
// =============================================================================
let turns = 
    userMsgs
    | join kind=inner modelMsgs on conversationId, messageId
    | join kind=leftouter tokenData on messageId
    | join kind=leftouter toolData on messageId
    | project 
        conversationId,
        userName,
        turnIndex,
        messageId,
        userMessage,
        modelMessage,
        llmCalls = coalesce(llmCalls, dynamic([])),
        turnSummary = pack(
            // ACTUAL tokens from LLM API (billed)
            "actual_API", pack(
                "maxPromptTokens_(system+user+assistant+toolResults)", coalesce(turnMaxPromptTokens, 0),
                "totalCompletionTokens", coalesce(turnTotalCompletionTokens, 0)
            ),
            "llmCallCount", array_length(coalesce(llmCalls, dynamic([]))),
            "hasTrajectory", coalesce(turnHasTrajectory, false)
        ),
        // Tools object - metadata about tool usage in this turn
        tools = pack(
            // Tool DEFINITIONS available (sent in system message each LLM call)
            "definitions", pack(
                "names", coalesce(availableTools, "[]"),
                "count", coalesce(availableToolCount, 0)
            ),
            // Tool INVOCATIONS (tools actually called during this turn)
            "invocations", pack(
                "withFrequency", coalesce(toolCounts, "{}")
            )
        ),
        availableToolCount = coalesce(availableToolCount, 0),
        numRequests = coalesce(numRequests, 0),
        turnDurationMs = coalesce(turnDuration, 0),
        turnEndReason = coalesce(turnEndReason, "unknown"),
        turnMaxPromptTokens = coalesce(turnMaxPromptTokens, 0);

// =============================================================================
// STEP 5: Aggregate to conversations with completeness & quality checks
// =============================================================================
let conversations = 
    turns
    | order by conversationId asc, turnIndex asc
    | summarize 
        userName = take_any(userName),
        capturedTurnCount = count(),
        minTurnIndex = min(turnIndex),
        maxTurnIndex = max(turnIndex),
        // ACTUAL tokens from LLM API (billed)
        // totalPromptTokens = sum of all turns' max promptTokens (includes: system + user + assistant + toolResults)
        totalPromptTokens_actual = sum(turnMaxPromptTokens),
        totalCompletionTokens_actual = sum(toint(turnSummary.actual_API.totalCompletionTokens)),
        // Track failed/cancelled/error turns for filtering
        hasFailedTurn = countif(turnEndReason == "failed") > 0,
        hasCancelledTurn = countif(turnEndReason == "cancelled") > 0,
        hasNetworkErrorTurn = countif(turnEndReason == "networkError") > 0,
        // Count turns with missing token data
        turnsWithEmptyLlmCalls = countif(array_length(llmCalls) == 0),
        turnsWithZeroTokens = countif(turnMaxPromptTokens == 0),
        turnsArray = make_list(pack(
            "turnIndex", turnIndex,
            "messageId", messageId,
            "userMessage", userMessage,
            "modelMessage", modelMessage,
            "llmCalls", llmCalls,
            "turnSummary", turnSummary,
            "tools", tools,
            "numRequests", numRequests,
            "turnDurationMs", turnDurationMs
        ))
        by conversationId
    | mv-apply turnsArray on (
        order by toint(turnsArray.turnIndex) asc
        | summarize turnsArray = make_list(turnsArray)
    )
    // ==========================================================================
    // QUALITY FILTERS
    // ==========================================================================
    // 1. COMPLETENESS CHECK: All turns must be present
    | where minTurnIndex == 1
    | where capturedTurnCount == maxTurnIndex
    // 2. NO FAILURES: Exclude conversations with any failed/cancelled/error turns
    | where hasFailedTurn == false
    | where hasCancelledTurn == false
    | where hasNetworkErrorTurn == false
    // 3. VALID TOKEN DATA: All turns must have LLM call data
    | where turnsWithEmptyLlmCalls == 0
    | where turnsWithZeroTokens == 0
    // ==========================================================================
    // Assign bucket
    | extend bucket = case(
        capturedTurnCount >= bucket_short_min and capturedTurnCount <= bucket_short_max, 
            "short_3_to_5_turns",
        capturedTurnCount >= bucket_medium_min and capturedTurnCount <= bucket_medium_max, 
            "medium_6_to_10_turns",
        capturedTurnCount >= bucket_long_min and capturedTurnCount <= bucket_long_max, 
            "long_11_to_20_turns",
        "excluded"
    )
    | where bucket != "excluded";

// =============================================================================
// RETURN ALL CANDIDATES (NO SAMPLING - done in Python)
// =============================================================================
conversations
| project 
    conversationId,
    userName,
    bucket,
    turnCount = capturedTurnCount,
    // Conversation-level token totals (ACTUAL from LLM API - billed)
    // totalPromptTokens = sum of (system + user + assistant + toolResults) across all turns
    totalPromptTokens_actual,
    totalCompletionTokens_actual,
    turnsArray
// ─────────────────────────────────────────────────────────────────────────────
// OUTPUT SCHEMA:
// ─────────────────────────────────────────────────────────────────────────────
// 
// Per llmCall (inside turnsArray[].llmCalls[]):
//
//   actual_API: {                          ← BILLED tokens from LLM API
//     promptTokens_(system+user+assistant+toolResults),  ← Input tokens (combined)
//     completionTokens,                    ← Output tokens from model
//     totalTokens                          ← promptTokens + completionTokens
//   }
//
//   estimates_Copilot: {                   ← Client-side ESTIMATES (for truncation decisions)
//     systemTokens,                        ← System prompt + tool definitions (CONSTANT in turn)
//     userTokens,                          ← User message (CONSTANT in turn)
//     assistantTokens,                     ← Model responses (GROWS in turn)
//     toolResultTokens,                    ← Tool outputs (GROWS in turn)
//     trajectoryTotal,                     ← Sum of all estimates
//     hasTrajectory                        ← true if trajectory data was captured
//   }
//
// Per turn (inside turnsArray[]):
//   turnSummary: {
//     actual_API: {
//       maxPromptTokens_(system+user+assistant+toolResults),  ← Max prompt tokens
//       totalCompletionTokens              ← Sum of completion tokens
//     },
//     llmCallCount,                        ← Number of LLM calls in this turn
//     hasTrajectory                        ← true if any call has trajectory
//   }
//
//   tools: {                               ← Tool metadata (from toolCallDetailsInternal event)
//     definitions: {                       ← Tools available in system message
//       names,                             ← JSON array: ["create_file","read_file",...]
//       count                              ← Number of tools (e.g., 78)
//     },
//     invocations: {                       ← Tools actually called in this turn
//       withFrequency                      ← JSON object: {"read_file":2,"write_file":1}
//     }
//   }
//
// COST ANALYSIS: Use actual_API fields (billed by LLM)
// COMPOSITION BREAKDOWN: Use estimates_Copilot fields (Copilot's tokenizer estimates)
// TOOL METADATA: See 03_TOOL_TELEMETRY.md for details
// ─────────────────────────────────────────────────────────────────────────────
