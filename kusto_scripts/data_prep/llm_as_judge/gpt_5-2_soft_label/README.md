# GPT-5.2 Soft Label Teacher

A modular async Python package for binary classification with soft-label generation
using Azure OpenAI with logprobs and logit_bias.

## Task Description

**Binary Classification**: "Does this user message require reasoning?"

| Label | Token | Meaning | Examples |
|-------|-------|---------|----------|
| 0 | `"0"` | **REQUIRES reasoning** | Complex refactoring, architecture decisions, multi-step debugging |
| 1 | `"1"` | **Does NOT require reasoning** | Simple syntax, code snippets, basic explanations |

## Key Features

- **Per-turn labeling**: Each user message in a conversation gets its own label
- **Soft labels**: Probability in [0, 1] derived from logprobs
- **Rationales by default**: Human-readable explanations (use `--no-rationales` to disable)
- **Modular strategies**: Plug-in system for what the LLM sees
- **Azure Blob input**: Read directly from blob storage
- **Tenacity retry**: Robust retry with exponential backoff
- **Rate limit aware**: Semaphore=50 based on gpt-5.2 deployment limits

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GPT-5.2 SOFT LABEL TEACHER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  config.py  â”‚   â”‚  client.py  â”‚   â”‚ tokenizer.pyâ”‚   â”‚  prompts.py â”‚     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚  â”‚ â€¢ Rate lims â”‚   â”‚ â€¢ Azure OAI â”‚   â”‚ â€¢ tiktoken  â”‚   â”‚ â€¢ System    â”‚     â”‚
â”‚  â”‚ â€¢ Retry cfg â”‚   â”‚ â€¢ Key Vault â”‚   â”‚ â€¢ Token IDs â”‚   â”‚ â€¢ Templates â”‚     â”‚
â”‚  â”‚ â€¢ Semaphore â”‚   â”‚ â€¢ tenacity  â”‚   â”‚ â€¢ Encoding  â”‚   â”‚ â€¢ Formattingâ”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                 â”‚                 â”‚                 â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                         classifier.py                                  â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚  classify_message()                                             â”‚  â”‚ â”‚
â”‚  â”‚  â”‚    â€¢ API call with logit_bias + logprobs                        â”‚  â”‚ â”‚
â”‚  â”‚  â”‚    â€¢ tenacity retry (exp backoff, 5 attempts)                   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚    â€¢ Extract soft_label = P("1") / (P("0") + P("1"))            â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                    â”‚                                        â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚         â”‚                          â”‚                          â”‚             â”‚
â”‚         â–¼                          â–¼                          â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚rationale.py â”‚          â”‚ pipeline.py â”‚          â”‚   cli.py    â”‚         â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
â”‚  â”‚ â€¢ Default ONâ”‚          â”‚ â€¢ Semaphore â”‚          â”‚ â€¢ argparse  â”‚         â”‚
â”‚  â”‚ â€¢ tenacity  â”‚          â”‚ â€¢ Async     â”‚          â”‚ â€¢ label     â”‚         â”‚
â”‚  â”‚ â€¢ Separate  â”‚          â”‚ â€¢ Progress  â”‚          â”‚ â€¢ info      â”‚         â”‚
â”‚  â”‚   call      â”‚          â”‚ â€¢ Stats     â”‚          â”‚ â€¢ test      â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                              io/                                       â”‚ â”‚
â”‚  â”‚  blob_reader.py : Read from Azure Blob Storage                        â”‚ â”‚
â”‚  â”‚  schemas.py     : TurnRecord, LabeledTurnRecord, ConversationRecord   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                           strategies/                                  â”‚ â”‚
â”‚  â”‚  base.py              : Abstract LabelingStrategy interface           â”‚ â”‚
â”‚  â”‚  user_message_only.py : Strategy A - classify message in isolation    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow

```
                         AZURE BLOB STORAGE
                    (github-copilot-sft-data-all-languages)
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           io/blob_reader.py                                â”‚
â”‚  download_split("train", dataset_name)                                     â”‚
â”‚    â†’ List[ConversationRecord]                                              â”‚
â”‚  flatten_to_turns(conversations)                                           â”‚
â”‚    â†’ List[TurnRecord]                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                      List[TurnRecord]
                   Each turn = one user message
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           strategies/                                       â”‚
â”‚  UserMessageOnlyStrategy.apply(turn)                                        â”‚
â”‚    â†’ StrategyResult(text_to_classify=turn.user_message)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                       text_to_classify
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          pipeline.py                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  FOR EACH TURN (async, semaphore=50):                                 â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  1. classify_message(text_to_classify)                               â”‚  â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚     â”‚  AZURE OPENAI API CALL (with tenacity retry)                â”‚  â”‚  â”‚
â”‚  â”‚     â”‚  â€¢ max_tokens=1                                             â”‚  â”‚  â”‚
â”‚  â”‚     â”‚  â€¢ logprobs=True, top_logprobs=5                            â”‚  â”‚  â”‚
â”‚  â”‚     â”‚  â€¢ logit_bias={15: 5.0, 16: 5.0}                            â”‚  â”‚  â”‚
â”‚  â”‚     â”‚  â€¢ temperature=1.0                                          â”‚  â”‚  â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  2. Extract hard_label from generated token                          â”‚  â”‚
â”‚  â”‚  3. Compute soft_label = P("1") / (P("0") + P("1"))                  â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  4. generate_rationale(text, label)  [DEFAULT: enabled]              â”‚  â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚     â”‚  SEPARATE API CALL (with tenacity retry)                    â”‚  â”‚  â”‚
â”‚  â”‚     â”‚  â€¢ max_tokens=128                                           â”‚  â”‚  â”‚
â”‚  â”‚     â”‚  â€¢ temperature=0.7                                          â”‚  â”‚  â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  5. Return LabeledTurnRecord                                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                          OUTPUT JSONL
                     (one line per turn)
```

## Output Schema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          OUTPUT JSONL SCHEMA                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  {                                                                          â”‚
â”‚    "conversationId": "abc-123-def",      // Links to original data         â”‚
â”‚    "messageId": "msg-456-xyz",           // Specific turn identifier       â”‚
â”‚    "split": "train",                     // Data split (train/val/test)    â”‚
â”‚    "bucket": "short_3_to_5_turns",       // Stratification bucket          â”‚
â”‚    "hard_label": 0,                      // 0=reasoning, 1=non-reasoning   â”‚
â”‚    "soft_label": 0.23,                   // P(non-reasoning) in [0, 1]     â”‚
â”‚    "rationale": "This message asks..."  // Human-readable explanation     â”‚
â”‚  }                                                                          â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  FIELD DESCRIPTIONS:                                                        â”‚
â”‚                                                                             â”‚
â”‚  IDENTIFIERS (for joining with original data):                              â”‚
â”‚    conversationId + messageId = Unique key                                  â”‚
â”‚                                                                             â”‚
â”‚  PROVENANCE (NOT passed to LLM judge):                                      â”‚
â”‚    split  : Data partition (train/val/test)                                 â”‚
â”‚    bucket : Stratification tier by conversation length                      â”‚
â”‚             â€¢ short_3_to_5_turns                                            â”‚
â”‚             â€¢ medium_6_to_10_turns                                          â”‚
â”‚             â€¢ long_11_to_20_turns                                           â”‚
â”‚                                                                             â”‚
â”‚  LABELS:                                                                    â”‚
â”‚    hard_label (int):                                                        â”‚
â”‚      - 0: Message REQUIRES reasoning (complex, multi-step)                  â”‚
â”‚      - 1: Message does NOT require reasoning (simple, pattern-matching)     â”‚
â”‚                                                                             â”‚
â”‚    soft_label (float):                                                      â”‚
â”‚      Probability of label 1 (non-reasoning) in [0, 1]                       â”‚
â”‚      - 0.0: Definitely requires reasoning                                   â”‚
â”‚      - 0.5: Uncertain                                                       â”‚
â”‚      - 1.0: Definitely does not require reasoning                           â”‚
â”‚      NOTE: P(reasoning) = 1 - soft_label                                    â”‚
â”‚                                                                             â”‚
â”‚  EXPLANATION:                                                               â”‚
â”‚    rationale (str):                                                         â”‚
â”‚      Human-readable explanation (2-4 sentences)                             â”‚
â”‚      Generated by default, disable with --no-rationales                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Blob Storage Organization

### Input (RAW) Data Structure

```
Container: github-copilot-sft-data-all-languages
â””â”€â”€ experiments/testvscode_test/v4/
    â””â”€â”€ vscodedata_120k_complete_stratified_deduped_60d_YYYYMMDD/  â† RAW
        â”œâ”€â”€ train/
        â”‚   â”œâ”€â”€ short_3_to_5_turns.json      (40k conversations)
        â”‚   â”œâ”€â”€ medium_6_to_10_turns.json    (40k conversations)
        â”‚   â””â”€â”€ long_11_to_20_turns.json     (20k conversations)
        â”œâ”€â”€ val/
        â”‚   â”œâ”€â”€ short_3_to_5_turns.json      (4k conversations)
        â”‚   â”œâ”€â”€ medium_6_to_10_turns.json    (4k conversations)
        â”‚   â””â”€â”€ long_11_to_20_turns.json     (2k conversations)
        â”œâ”€â”€ test/
        â”‚   â”œâ”€â”€ short_3_to_5_turns.json      (4k conversations)
        â”‚   â”œâ”€â”€ medium_6_to_10_turns.json    (4k conversations)
        â”‚   â””â”€â”€ long_11_to_20_turns.json     (2k conversations)
        â””â”€â”€ metadata.json
```

### Output (LABELED) Data Structure

âš ï¸ **SAFETY PRINCIPLE: We NEVER touch the RAW data folder.**
We create a parallel folder with `LABELED_` prefix at the same level.

```
Container: github-copilot-sft-data-all-languages
â””â”€â”€ experiments/testvscode_test/v4/
    â”‚
    â”œâ”€â”€ vscodedata_120k_..._YYYYMMDD/              â† RAW (READ ONLY)
    â”‚   â””â”€â”€ (original structure preserved)
    â”‚
    â””â”€â”€ LABELED_vscodedata_120k_..._YYYYMMDD/      â† NEW (WRITE)
        â”œâ”€â”€ train/
        â”‚   â”œâ”€â”€ short_3_to_5_turns.jsonl           â† Per-turn labels
        â”‚   â”œâ”€â”€ medium_6_to_10_turns.jsonl
        â”‚   â””â”€â”€ long_11_to_20_turns.jsonl
        â”œâ”€â”€ val/
        â”‚   â”œâ”€â”€ short_3_to_5_turns.jsonl
        â”‚   â”œâ”€â”€ medium_6_to_10_turns.jsonl
        â”‚   â””â”€â”€ long_11_to_20_turns.jsonl
        â”œâ”€â”€ test/
        â”‚   â”œâ”€â”€ short_3_to_5_turns.jsonl
        â”‚   â”œâ”€â”€ medium_6_to_10_turns.jsonl
        â”‚   â””â”€â”€ long_11_to_20_turns.jsonl
        â””â”€â”€ labeling_metadata.json
```

### Why This Approach?

| Property | Benefit |
|----------|---------|
| **No modifications to RAW** | Zero risk of data loss/corruption |
| **No copy/move operations** | Avoids mid-transfer failures |
| **LABELED_ prefix** | Clear naming, easy discovery |
| **Mirrored structure** | Same split/bucket organization |
| **Easy joins** | Parallel paths enable simple matching |

### File Format Difference

| Folder | Format | Content |
|--------|--------|---------|
| RAW | `.json` | Array of conversations (nested turns) |
| LABELED | `.jsonl` | One line per **turn** (flattened) |

## Tokenizer Explained

### What's a Token?
LLMs see text as **tokens** â€” chunks that might be words, parts of words, or characters:
```
"Hello world" â†’ ["Hello", " world"] â†’ Token IDs: [9906, 1917]
"0"           â†’ ["0"]               â†’ Token ID: [15]
"1"           â†’ ["1"]               â†’ Token ID: [16]
```

### Why Token IDs?
The `logit_bias` API parameter requires token IDs (integers), not strings:
```python
# We need to boost tokens "0" and "1"
logit_bias = {15: 5.0, 16: 5.0}  # Token IDs, not strings!
```

### How We Get Them
We use `tiktoken` (OpenAI's tokenizer):
```python
import tiktoken
enc = tiktoken.get_encoding("cl100k_base")  # GPT-4/5 encoding
enc.encode("0")  # Returns [15]
enc.encode("1")  # Returns [16]
```

## Semaphore & Rate Limits

### Why Semaphores with Async?

**Async â‰  Concurrency Control**

Async allows non-blocking I/O, but doesn't limit how many requests we start:

```python
# WITHOUT semaphore: All 100k requests start at once â†’ RATE LIMIT ERROR
tasks = [call_api(msg) for msg in messages]
await asyncio.gather(*tasks)  # ğŸ’¥

# WITH semaphore: Only 50 run at a time
semaphore = asyncio.Semaphore(50)
async with semaphore:
    await call_api(msg)  # âœ…
```

### Rate Limit Analysis for gpt-5.2 Deployment

**Azure OpenAI Deployment Limits (Global Standard tier):**

| Limit | Value |
|-------|-------|
| Requests per minute (RPM) | 10,000 |
| Tokens per minute (TPM) | 1,000,000 |

**Token Usage per Request:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLASSIFICATION CALL                                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  System prompt .......................... ~100 tokens                       â”‚
â”‚  User message (avg) ..................... ~100 tokens                       â”‚
â”‚  Output (max_tokens=1) .................. 1 token                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚  TOTAL â‰ˆ 200 tokens/request                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RATIONALE CALL (separate request)                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  System prompt .......................... ~50 tokens                        â”‚
â”‚  User message + label ................... ~100 tokens                       â”‚
â”‚  Output (max_tokens=128) ................ ~100 tokens avg                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚  TOTAL â‰ˆ 250 tokens/request                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  COMBINED (with rationales enabled) â‰ˆ 450 tokens/turn                       â”‚
â”‚  CLASSIFICATION ONLY â‰ˆ 200 tokens/turn                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Rate Limit Math:**

```
MODE 1: Classification Only (--no-rationales)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Token limit:    1,000,000 TPM Ã· 200 tokens = 5,000 req/min
Request limit:  10,000 RPM
Bottleneck:     Request limit (can do 10K/min)
Safe rate:      ~5,000 req/min (50% headroom)

With 50 concurrent + ~0.6s latency:
  50 concurrent Ã— (60s Ã· 0.6s) = 5,000 req/min âœ“


MODE 2: With Rationales (default)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2 API calls per turn: classify + rationale
Tokens per turn:  ~450 tokens
Token limit:      1,000,000 TPM Ã· 450 = 2,222 turns/min
Request limit:    10,000 RPM Ã· 2 = 5,000 turns/min
Bottleneck:       Token limit (2,222 turns/min)
Safe rate:        ~1,500 turns/min (30% headroom)

With 50 concurrent + ~2s total latency (both calls):
  50 concurrent Ã— (60s Ã· 2s) = 1,500 turns/min âœ“
```

**Semaphore Selection:**

| Concurrency | Classification Only | With Rationales |
|-------------|---------------------|-----------------|
| 20 | ~2,000/min | ~600/min |
| **50** | **~5,000/min** | **~1,500/min** |
| 100 | ~10,000/min (risky) | ~3,000/min (risky) |

**Recommendation: semaphore=50** (default)
- Safe for both modes
- 30-50% headroom for latency spikes
- Tenacity retry handles 429 errors gracefully

**Estimated Processing Time (120K turns):**

| Mode | Rate | Time |
|------|------|------|
| Classification only | 5,000/min | ~24 minutes |
| With rationales | 1,500/min | ~80 minutes |

## Module Structure

```
gpt_5-2_soft_label/
â”œâ”€â”€ __init__.py        # Package exports
â”œâ”€â”€ __main__.py        # Entry point: python -m gpt_5-2_soft_label
â”œâ”€â”€ config.py          # Rate limits, retry config, blob settings
â”œâ”€â”€ client.py          # Azure OpenAI client + Key Vault
â”œâ”€â”€ tokenizer.py       # tiktoken token ID resolution
â”œâ”€â”€ prompts.py         # System prompts and templates
â”œâ”€â”€ classifier.py      # Classification with logprobs + tenacity
â”œâ”€â”€ rationale.py       # Rationale generation + tenacity
â”œâ”€â”€ pipeline.py        # Async processing with semaphore
â”œâ”€â”€ cli.py             # Command-line interface
â”œâ”€â”€ README.md          # This documentation
â”‚
â”œâ”€â”€ io/                # Input/Output handling
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ blob_reader.py # Read RAW data from Azure Blob Storage
â”‚   â”œâ”€â”€ blob_writer.py # Write LABELED data (safe, parallel folder)
â”‚   â””â”€â”€ schemas.py     # TurnRecord, LabeledTurnRecord, provenance
â”‚
â””â”€â”€ strategies/        # Labeling strategies
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ base.py        # Abstract strategy interface
    â””â”€â”€ user_message_only.py  # Strategy A
```

## Installation

```bash
cd kusto_scripts/data_prep/llm_as_judge
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## Usage

### Label a Dataset (Rationales Enabled by Default)

```bash
python -m gpt_5-2_soft_label label \
    --input conversations.jsonl \
    --output labeled.jsonl \
    --model gpt-5.2
```

### Label Without Rationales (Faster, Cheaper)

```bash
python -m gpt_5-2_soft_label label \
    --input conversations.jsonl \
    --output labeled.jsonl \
    --model gpt-5.2 \
    --no-rationales
```

### Show Configuration

```bash
python -m gpt_5-2_soft_label info
```

### Test API Connectivity

```bash
python -m gpt_5-2_soft_label test \
    --message "How do I refactor this to use dependency injection?"
```

## Authentication

Priority order:
1. **Azure Key Vault** (production): Vault `abadawikeys`, Secret `gpt-5-2-api-key`
2. **Environment Variable** (development): `AZURE_OPENAI_API_KEY`

```bash
# Use Key Vault (default)
az login
python -m gpt_5-2_soft_label label ...

# Use env var only
export AZURE_OPENAI_API_KEY="your-key"
python -m gpt_5-2_soft_label label --no-keyvault ...
```

## Retry Logic (Tenacity)

All API calls use tenacity for robust retry:

| Setting | Value |
|---------|-------|
| Max retries | 5 |
| Min wait | 1 second |
| Max wait | 60 seconds |
| Backoff | Exponential (2x) |
| Retry on | 429, 500, 502, 503, 504 |

## Strategies

Strategies define **what the LLM sees** when classifying:

| Strategy | Description | Tokens |
|----------|-------------|--------|
| `user_message_only` | Just the user message, no context | Minimal |
| *(future)* `with_context` | Include previous turns | More |
| *(future)* `with_response` | Include model's response | More |

## Cost Estimation

| Mode | Tokens/turn | Cost @ $0.01/1k | 120k turns |
|------|-------------|-----------------|------------|
| Classification only | ~200 | $0.002 | $240 |
| With rationales | ~350 | $0.0035 | $420 |

## Troubleshooting

### "tiktoken not installed"
```bash
pip install tiktoken
```

### Rate limit errors
```bash
# Reduce concurrency
python -m gpt_5-2_soft_label label --concurrency 20 ...
```

### "No label in top_logprobs"
This is rare with logit_bias. Check:
- logit_bias is being applied
- Temperature is 1.0
- Model deployment supports logprobs
