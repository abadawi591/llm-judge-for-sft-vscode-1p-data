"""
Classifier Module
=================

Core classification logic with soft-label extraction from logprobs.

This module implements the main classification function that:
    1. Makes an async API call with max_tokens=1
    2. Uses logit_bias to concentrate probability on "0"/"1" tokens
    3. Extracts raw probabilities from top_logprobs
    4. Renormalizes to compute soft label in [0, 1]
    5. Uses tenacity for robust retry with exponential backoff

Soft Label Theory:
    The model outputs a probability distribution over all tokens. With
    logit_bias, we shift this distribution heavily toward "0" and "1",
    but the relative preference between them remains meaningful.
    
    By extracting P("0") and P("1") from logprobs and renormalizing:
        soft_label = P("1") / (P("0") + P("1"))
    
    We get a calibrated probability that can be used for:
    - Confidence estimation
    - Uncertainty quantification
    - Knowledge distillation
    - Active learning sample selection

Mathematical Details:
    Let L_0, L_1 be the original logits for tokens "0" and "1".
    With logit_bias b, the new logits are L_0 + b and L_1 + b.
    
    The probability ratio remains unchanged:
        P("1")/P("0") = exp(L_1)/exp(L_0) = exp(L_1 - L_0)
    
    Adding the same bias to both doesn't change the ratio, it just
    concentrates mass away from other tokens.

Retry Strategy:
    Uses tenacity for robust retry with:
    - Exponential backoff (1s → 2s → 4s → ... up to 60s)
    - Retry on rate limits (429) and server errors (5xx)
    - Max 5 retry attempts
"""

import math
from dataclasses import dataclass
from typing import Optional

from openai import AsyncAzureOpenAI, APIError, RateLimitError, APIConnectionError
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)

from .config import (
    CLASSIFICATION_MAX_TOKENS,
    CLASSIFICATION_TEMPERATURE,
    CLASSIFICATION_TOP_LOGPROBS,
    DEFAULT_LOGIT_BIAS,
    MAX_RETRIES,
    RETRY_MIN_WAIT,
    RETRY_MAX_WAIT,
    RETRY_MULTIPLIER,
)
from .prompts import get_classification_messages
from .tokenizer import LabelTokenizer


@dataclass
class ClassificationResult:
    """
    Result of a soft-label classification.
    
    Attributes:
        hard_label: Binary label (0 or 1)
            - 0 = requires reasoning (complex, multi-step)
            - 1 = does not require reasoning (simple, pattern-matching)
        
        soft_label: Probability for label 1 (non-reasoning) in [0, 1]
            - 0.0 = definitely requires reasoning
            - 0.5 = uncertain
            - 1.0 = definitely does not require reasoning
            Note: P(reasoning) = 1 - soft_label
        
        raw_prob_0: Raw probability mass for "0" from logprobs (before renorm)
        raw_prob_1: Raw probability mass for "1" from logprobs (before renorm)
        generated_token: The actual token generated by the model
        fallback_used: Whether fallback logic was used (no label in top_logprobs)
    
    Example:
        >>> result = await classify_message(client, "Hello", model, tokenizer)
        >>> print(f"Hard label: {result.hard_label}")
        >>> print(f"Soft label: {result.soft_label:.3f}")
        >>> print(f"P(reasoning): {1 - result.soft_label:.3f}")
    """
    hard_label: int
    soft_label: float  # P(non-reasoning) = P(label=1)
    raw_prob_0: float
    raw_prob_1: float
    generated_token: str
    fallback_used: bool
    
    @property
    def confidence(self) -> float:
        """Confidence in the hard label (distance from 0.5)."""
        return abs(self.soft_label - 0.5) * 2  # Maps [0,1] to [0,1] with 0.5->0
    
    def is_uncertain(self, threshold: float = 0.3) -> bool:
        """Whether the classification is uncertain (close to 0.5)."""
        return self.confidence < threshold
    
    def to_dict(self) -> dict:
        """Convert to minimal output dict for JSON serialization."""
        return {
            "hard_label": self.hard_label,
            "soft_label": round(self.soft_label, 4),  # 4 decimal places
        }
    
    def to_full_dict(self) -> dict:
        """Convert to full dict with all metadata (for debugging)."""
        return {
            "hard_label": self.hard_label,
            "soft_label": round(self.soft_label, 4),
            "confidence": round(self.confidence, 4),
            "raw_prob_0": round(self.raw_prob_0, 6),
            "raw_prob_1": round(self.raw_prob_1, 6),
            "generated_token": self.generated_token,
            "fallback_used": self.fallback_used,
        }


# Retry decorator for API calls
_retry_decorator = retry(
    stop=stop_after_attempt(MAX_RETRIES),
    wait=wait_exponential(
        multiplier=RETRY_MULTIPLIER,
        min=RETRY_MIN_WAIT,
        max=RETRY_MAX_WAIT,
    ),
    retry=retry_if_exception_type((RateLimitError, APIConnectionError, APIError)),
    reraise=True,
)


@_retry_decorator
async def _call_classification_api(
    client: AsyncAzureOpenAI,
    messages: list,
    model: str,
    logit_bias: dict,
) -> dict:
    """
    Make the classification API call with retry logic.
    
    This is separated from the main function to apply tenacity retry
    decorator specifically to the API call, not the result processing.
    
    Args:
        client: AsyncAzureOpenAI client
        messages: Chat messages list
        model: Deployment name
        logit_bias: Token ID to bias value mapping
    
    Returns:
        API response object
    
    Raises:
        RateLimitError: After max retries on 429
        APIError: After max retries on other API errors
    """
    response = await client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=CLASSIFICATION_MAX_TOKENS,
        temperature=CLASSIFICATION_TEMPERATURE,
        logprobs=True,
        top_logprobs=CLASSIFICATION_TOP_LOGPROBS,
        logit_bias=logit_bias,
    )
    return response


async def classify_message(
    client: AsyncAzureOpenAI,
    user_message: str,
    model: str,
    tokenizer: LabelTokenizer,
    logit_bias_value: float = DEFAULT_LOGIT_BIAS,
) -> ClassificationResult:
    """
    Classify a user message and compute soft label from logprobs.
    
    This function:
    1. Generates exactly one token (max_tokens=1)
    2. Uses logit_bias to favor "0" and "1" tokens
    3. Extracts probabilities from top_logprobs
    4. Computes soft label via renormalization
    5. Retries on rate limits and server errors (via tenacity)
    
    Args:
        client: AsyncAzureOpenAI client instance
        user_message: The user message to classify
        model: Azure deployment name (e.g., "gpt-5.2")
        tokenizer: LabelTokenizer with token IDs for "0" and "1"
        logit_bias_value: Bias to apply to label tokens (default: 5.0)
    
    Returns:
        ClassificationResult with:
        - hard_label: 0 or 1
        - soft_label: P(non-reasoning) in [0, 1]
        - metadata for debugging
    
    Raises:
        RateLimitError: After max retries exhausted
        APIError: On unrecoverable API errors
    
    Example:
        >>> result = await classify_message(
        ...     client=client,
        ...     user_message="How do I sort a list?",
        ...     model="gpt-5.2",
        ...     tokenizer=tokenizer,
        ... )
        >>> print(f"Label: {result.hard_label}")
        >>> print(f"P(non-reasoning): {result.soft_label:.3f}")
    
    Note:
        The logit_bias of +5 multiplies odds of "0" and "1" by exp(5) ≈ 148x,
        concentrating probability mass on these labels while preserving the
        model's relative preference between them.
    """
    # Build logit_bias dict
    # +5 logit bias multiplies the odds of "0" and "1" by exp(5) ~ 148x,
    # concentrating probability mass on these labels while keeping a soft
    # preference between them.
    logit_bias = tokenizer.get_logit_bias(logit_bias_value)
    
    # Build messages
    messages = get_classification_messages(user_message)
    
    # Make API call with retry
    response = await _call_classification_api(client, messages, model, logit_bias)
    
    # Extract generated token
    choice = response.choices[0]
    generated_token = choice.message.content.strip() if choice.message.content else ""
    
    # Determine hard label from generated token
    if generated_token.startswith("1"):
        hard_label = 1
    else:
        hard_label = 0  # Default to 0 for any non-"1" output
    
    # Extract probabilities from logprobs
    # The logprobs structure is: choice.logprobs.content[0].top_logprobs
    raw_prob_0 = 0.0
    raw_prob_1 = 0.0
    
    if choice.logprobs and choice.logprobs.content:
        top_logprobs = choice.logprobs.content[0].top_logprobs
        
        for item in top_logprobs:
            token = item.token.strip()
            prob = math.exp(item.logprob)
            
            if token == "0":
                raw_prob_0 = prob
            elif token == "1":
                raw_prob_1 = prob
    
    # Compute soft label via renormalization
    # We now have raw probability mass assigned to "0" and "1".
    # The model still assigned probability to other tokens, but we ignore those
    # and renormalize over {0, 1} only to get a clean soft label:
    fallback_used = False
    
    if raw_prob_0 == 0.0 and raw_prob_1 == 0.0:
        # Fallback: if neither 0 nor 1 appeared in top_logprobs,
        # use the hard label as a degenerate probability.
        soft_label = float(hard_label)
        fallback_used = True
    else:
        total = raw_prob_0 + raw_prob_1
        if total == 0.0:
            soft_label = 0.5  # Edge case: shouldn't happen
            fallback_used = True
        else:
            soft_label = raw_prob_1 / total
    
    return ClassificationResult(
        hard_label=hard_label,
        soft_label=soft_label,
        raw_prob_0=raw_prob_0,
        raw_prob_1=raw_prob_1,
        generated_token=generated_token,
        fallback_used=fallback_used,
    )


# Alias for backward compatibility
get_soft_label_for_message = classify_message
